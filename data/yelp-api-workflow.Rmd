---
title: "Data Collection: Yelp Fusion API"
output: html_document
theme: "flatly"
date: "September 9, 2024"
author: Anna Duan, annaduan@sas.upenn.edu
---
# Introduction  
This markdown gathers business listing data from Yelp Fusion API's Business Search Endpoint. We use OmaymaS's [`yelpr`](https://github.com/OmaymaS/yelpr) package to make the API call itself, and `httr` to parse through the response. We also use `tigris` to generate a list of all ZIP codes in Philadelphia to use as location specifications for the API call, then we later use these boundaries to filter out businesses that are not within the city boundaries.  
  
To use [Yelp Fusion API](https://docs.developer.yelp.com/docs/fusion-intro), first register for a Yelp account, then navigate to [manage API access](https://www.yelp.com/login?return_url=/developers/v3/manage_app) to find your API key.  
  
## Libraries  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      eval = FALSE)
# libraries 
library(devtools)
library(tigris)
library(tidyverse)
library(sf)
library(httr)

devtools::install_github("OmaymaS/yelpr")
library(yelpr)
```

## Tigris boundaries 
Using `tigris`, we access ZIP code boundaries and names from the US Census Bureau. Using the resulting sf object, we create a list of Philadelphia ZIP codes and a unioned Philadelphia boundary object.
```{r load tigris boundaries}
options(tigris_use_cache = TRUE) # set cache = TRUE to save time for future calls

zcta <- zctas(year = 2021) %>% # get all ZIPs in US (we can't filter by state unfortunately)
  rename(zip_code = GEOID20) %>% 
  st_transform("EPSG:2272") %>% # re-project
  filter(substr(ZCTA5CE20, start = 1, stop = 3) == "191") %>%
  erase_water(area_threshold = 0.5)

zcta_bg <- zctas(year = 2021) %>% # get all ZIPs in US
  rename(zip_code = GEOID20) %>% 
  st_transform("EPSG:2272") %>% # re-project
  st_crop(st_bbox(zcta %>% st_buffer(5000))) %>%
  erase_water(area_threshold = 0.5)

zip_list <- zcta$ZCTA5CE20 # list of zip codes to include in API query

phl_bound <- st_union(zcta) %>% # phl boundary
  st_as_sf()
```

# Yelp Fusion API  
## Helper function - `get_yelp()`  
To parse the results of the API call, a JSON file, we use a helper function to extract the fields we need. After creating a list of the specifications for the API call, we use `httr::GET()` to make the call. The result is a JSON file, which we flatten and and parse using `httr::content()`.   

We then retrieve specific columns, such as business name, latitude, longitude, alias, and rating as dataframes which we later concatenate into a larger dataframe.    

```{r get_yelp helper function}
#### Yelp api call function ####
url <- "https://api.yelp.com/v3/businesses/search"

get_yelp = function(category, zip_code, offset_num) {
  # args are category of business, zipcode, and number to offset by
  
  queryString = list(
    location = zip_code,
    # argument to be filled
    term = category,
    # argument to be filled
    sort_by = "distance",
    # sort by dist
    limit = 50,
    # 50 is the max for yelp fusion api, any higher and it won't work
    offset = offset_num # argument to be filled
  )
  
  # use "GET" verb to request information from url
  response <- VERB(
    "GET",
    url,
    query = queryString,
    add_headers('Authorization' = 'Bearer u6vZZfFrAnI1IHH0p40WwmEB2W8yp8l-r1m8lmLmoYJCuq8obxrgyNYsgRJXwkqD1E4ceSmLuMEIuKUwF0yvv0Pb1GOO2TwtCaDfKl1DJ_L9xSElW4kCigyWG3tCZXYx'),
    content_type("application/octet-stream"),
    accept("application/json")
  )
  
  # turn the response into a json file
  yelp.json = content(response, "parsed", flatten = TRUE, simplify = TRUE)

  # retrieve columns from json structure
  biz.name = data.frame(yelp.json$businesses$name)
  biz.lat = data.frame(yelp.json$businesses$coordinates.latitude)
  biz.lon = data.frame(yelp.json$businesses$coordinates.longitude)
  biz.rating = data.frame(yelp.json$businesses$rating)
  biz.addr = data.frame(yelp.json$businesses$location.address1)
  
  # bind the columns into one dataframe
  yelp.df = cbind(biz.name, biz.rating, biz.addr, biz.lat, biz.lon)  %>%
    as.data.frame()
  
  colnames(yelp.df) <- c("name", "rating", "address", "lat", "lon")
  
  # add in category alias/title (this will give us cuisine information)
  cuisine = yelp.json$businesses$categories
  
  cuis.df <- map_dfr(cuisine, function(x) {
    tibble(
      alias = paste(x$alias, collapse = ", "),
      title = paste(x$title, collapse = ", ")) %>%
      as.data.frame()
  })
  
  yelp.df <- yelp.df %>%
    cbind(cuis.df)
  
  # When creating an empty dataframe, use "" as default value
  if(nrow(yelp.df) == 0) {
    yelp.df <- data.frame(name="", rating=numeric(0), address="", lat=numeric(0), lon=numeric(0), alias = "", title = "", stringsAsFactors=FALSE)
  }
  
  return(yelp.df)
}
```

## Making the API call  
To make the API call, we write a nested dataframe to get around the 50 business limit. In order to capture all of the businesses in the city, we iterate through our list of ZIP codes, for which we iterate through offset values 0 to 10. This way, we can capture a maximum of 500 businesses per ZIP code, which is more than enough for most categories. For each loop, we store the output in a nested dataframe.      
```{r call api}
# Initialize a named list of empty dataframes
initialize_named_dfs <- function(zips) { 
  empty_df <- data.frame(name=character(0), rating=numeric(0),  address=character(0), lat=numeric(0), lon=numeric(0))
  named_list <- lapply(zips, function(zip) empty_df)
  names(named_list) <- zips
  return(named_list)
}

biz_list_0 <- initialize_named_dfs(zip_list) # Initiate list of restaurant dataframes for each offset (since the query limit is 50, offset = 1 would return 51-100)
biz_list_1 <- initialize_named_dfs(zip_list) # 10 dfs x 50 restaurants each = a 500 restaurant sample per zip code
biz_list_2 <- initialize_named_dfs(zip_list)
biz_list_3 <- initialize_named_dfs(zip_list)
biz_list_4 <- initialize_named_dfs(zip_list)
biz_list_5 <- initialize_named_dfs(zip_list)
biz_list_6 <- initialize_named_dfs(zip_list)


# master list to store the dataframes
offset_list <- list(biz_list_0,
                    biz_list_1,
                    biz_list_2,
                    biz_list_3,
                    biz_list_4,
                    biz_list_5,
                    biz_list_6)

# Loop through each offset (think of each offset as a page of results)
for (i in 1:length(zip_list)) {
  # initialize zipnum (this is so we know where we're at in the list of zips)
  zipnum <- 1
  # initialize offset (so we can pull page 1, then page 2, then page 3, etc)
  offset <- i - 1
  
  # Loop through each zip code
  for (zip_code in zip_list) {
    print(paste("batch ", offset + 1, ", ", "zip", zipnum, ": ", zip_code, sep = ""))
    
    # Fetch Yelp data for the zip code and store it in the list
    offset_list[[i]][[as.character(zip_code)]] <- get_yelp("restaurants", as.character(zip_code), offset)
    
    zipnum <- zipnum + 1 #iterate zipnum each loop
  }
  offset <- offset + 50
}
```

# Data preparation    
## Store response as dataframe   
In this step, we take the nested dataframe from the last step and bind all of the dataframes inside to create one flattened dataframe. Using the longitude and latitude fields we got earlier, we transform it into a spatial features object with point locations for all of the businesses inside.   

```{r gather data to df}
# Combine all dataframes into one dataframe and remove duplicates
biz.sf <- map_dfr(offset_list, ~ bind_rows(.x)) %>%
  unique() %>%
  filter(!is.na(lat)) %>%
  st_as_sf(crs = 4326, coords = c("lon", "lat")) %>% 
  st_transform("EPSG:2272")

```

## Write geojson file  
Before writing out the file, let's preview it using `mapview`:
```{r preview}
mapview(biz.sf)
```

After removing duplicates, we write the sf object out to a GeoJSON file for analysis.
```{r write files}
st_write(biz.sf, "restaurants.geojson", driver = "geojson")
```